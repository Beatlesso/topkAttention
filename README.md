# topkAttention
目前pytorch版本的实现在MyAttention.py中，总共三个版本，目前测试三个结果是一致的
第一个版本和暴力无差别，最能保证正确性，需要额外topk倍的显存，速度很慢，在batch为8的情况下需要30s
第二个版本避免了显示存储k和v，避免了额外的显存，batch可以设置到160，但是速度比第一个版本还要慢，在batch为8的情况下需要60s
第三个版本速度最快，但是仍然无法避免topk倍的额外显存占用，batch最大只能设置为8，但是速度很快，只需要0.11s

测试使用的代码在test.py中